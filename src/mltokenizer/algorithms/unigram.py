from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

from loguru import logger

from mltokenizer.core.base_tokenizer import (
    BaseTokenizer,
    TokenizedOutput,
    TokenizerOptions,
    TokenizerType,
)
from mltokenizer.core.errors import TokenizationError, UntrainedTokenizerError
from mltokenizer.encoding.encoder import Encoder
from mltokenizer.encoding.special_tokens import SpecialTokens
from mltokenizer.normalization.normalizers import Normalizer
from mltokenizer.preprocessing.pipeline import PreprocessingPipeline
from .sentencepiece_tokenizer import SentencePieceTokenizer # Import parent class

class UnigramTokenizer(SentencePieceTokenizer):
    """
    Unigram Language Model Tokenizer, implemented as a specialization of SentencePieceTokenizer.
    It uses SentencePiece library with the model_type set to 'unigram'.
    """

    def __init__(
        self,
        model_path: Optional[Union[str, Path]] = None,
        vocab_size: int = 8000, # Default for Unigram, but SentencePieceTokenizer default is 32000
        normalizer: Optional[Normalizer] = None,
        preprocessor: Optional[PreprocessingPipeline] = None,
        special_tokens: Optional[SpecialTokens] = None,
        options: Optional[TokenizerOptions] = None,
        unk_token: str = "<unk>",
        bos_token: str = "<s>",
        eos_token: str = "</s>",
        pad_token: str = "<pad>",
    ):
        """
        Initializes the UnigramTokenizer.

        Args:
            model_path: Path to a pre-trained SentencePiece Unigram model file (.model).
            vocab_size: Vocabulary size to use if training. Default is 8000.
            normalizer: Custom normalizer instance. SentencePiece models often handle normalization.
            preprocessor: Custom preprocessor pipeline instance.
            special_tokens: SpecialTokens handler instance.
            options: TokenizerOptions instance.
            unk_token: String for the unknown token.
            bos_token: String for the beginning-of-sequence token.
            eos_token: String for the end-of-sequence token.
            pad_token: String for the padding token.
        """
        # Initialize SpecialTokens if not provided, with Unigram-common defaults
        st_handler = special_tokens or SpecialTokens(
            unk_token=unk_token,
            bos_token=bos_token,
            eos_token=eos_token,
            pad_token=pad_token,
        )
        
        super().__init__(
            model_path=model_path,
            vocab_size=vocab_size,
            normalizer=normalizer,
            preprocessor=preprocessor,
            # Encoder will be built by parent's load_model
            special_tokens=st_handler,
            options=options,
            # Pass unk_token, bos_token etc. directly to parent which handles them
            unk_token=unk_token,
            bos_token=bos_token,
            eos_token=eos_token,
            pad_token=pad_token,
        )
        
        # Override tokenizer_type after parent initialization
        self.tokenizer_type = TokenizerType.UNIGRAM
        
        # _is_trained and self.sp_model (formerly self.model) are handled by parent
        logger.debug(f"Initialized UnigramTokenizer (derived from SentencePieceTokenizer). Trained: {self.is_trained}")

    def train(
        self, 
        texts: List[str], 
        model_prefix: str = "unigram_sp_model", # Default model prefix for unigram
        vocab_size: Optional[int] = None, # Allow overriding vocab_size for training
        **kwargs
    ) -> None:
        """
        Trains a Unigram model using SentencePiece.

        Args:
            texts: A list of strings representing the training corpus.
            model_prefix: Prefix for the model and vocab files generated by SentencePiece.
                          Defaults to "unigram_sp_model".
            vocab_size: The target vocabulary size. If None, uses the vocab_size
                        set during tokenizer initialization (default 8000 for Unigram).
            **kwargs: Additional arguments to be passed to SentencePieceTrainer.train().
                      For example, character_coverage, normalization_rule_name, etc.
                      `model_type` will be forced to "unigram".
        """
        # Use the vocab_size specified during training, or fall back to the instance's vocab_size
        effective_vocab_size = vocab_size if vocab_size is not None else self.vocab_size

        logger.info(f"Starting Unigram model training with model_prefix='{model_prefix}', vocab_size={effective_vocab_size}.")
        
        # Ensure model_type is unigram, overriding any user-passed kwarg
        kwargs["model_type"] = "unigram"
        
        try:
            super().train(
                texts=texts,
                model_prefix=model_prefix,
                vocab_size=effective_vocab_size, # Pass the resolved vocab_size
                **kwargs # Pass other SP training params
            )
            # After super().train(), self.load_model() is called, which sets _is_trained and _trained_model_path
            logger.info(f"Unigram model training completed using SentencePieceTokenizer.train. Model path: {self._trained_model_path}")
        except TokenizationError as e: # Catch errors from parent train
            logger.error(f"Unigram training failed: {e}", exc_info=True)
            raise # Re-raise the error
        except Exception as e:
            logger.error(f"An unexpected error occurred during Unigram training: {e}", exc_info=True)
            raise TokenizationError(f"Unigram training encountered an unexpected error: {e}")

    def encode(
        self, 
        text: str, 
        text_pair: Optional[str] = None,
        add_special_tokens: bool = True,
        return_tokens: bool = False
    ) -> TokenizedOutput:
        if not self.is_trained or not self.model:
            raise UntrainedTokenizerError("Unigram tokenizer is not trained or model not loaded.")

        # Normalization should ideally be part of the SentencePiece model itself if `normalization_rule_name` was used during training.
        # If not, apply our normalizer here.
        normalized_text = self.normalizer.normalize(text) if self.normalizer else text
        # Preprocessing pipeline
        processed_text = self.preprocessor.process(normalized_text) if self.preprocessor else normalized_text

        # SentencePiece handles add_bos/add_eos through encode options if model has them
        # For this generic wrapper, we manage special tokens more manually if `add_special_tokens` is true.
        
        # Encode single text or first part of a pair
        ids_sequence_1 = self.model.encode_as_ids(processed_text)
        tokens_sequence_1 = self.model.encode_as_pieces(processed_text) if return_tokens else None

        ids_sequence_2: Optional[List[int]] = None
        tokens_sequence_2: Optional[List[str]] = None
        if text_pair:
            normalized_text_pair = self.normalizer.normalize(text_pair) if self.normalizer else text_pair
            processed_text_pair = self.preprocessor.process(normalized_text_pair) if self.preprocessor else normalized_text_pair
            ids_sequence_2 = self.model.encode_as_ids(processed_text_pair)
            if return_tokens: tokens_sequence_2 = self.model.encode_as_pieces(processed_text_pair)
        
        # --- Combine and add special tokens (BOS/EOS, or CLS/SEP like for BERT if desired for consistency) ---
        # This part needs to be adapted based on how Unigram is typically used (e.g., with BOS/EOS for generation)
        # or if we want to force a BERT-like CLS/SEP structure for compatibility.
        # For now, let's assume BOS/EOS if available, and CLS/SEP are less common for raw Unigram.

        final_input_ids = []
        final_token_type_ids = [] # Usually 0 for Unigram unless used in BERT-like pair tasks
        final_special_tokens_mask = []
        final_output_tokens_list = [] if return_tokens else None

        # Special token handling (example: BOS/EOS for single, CLS/SEP for pair for illustration)
        # This logic should be refined based on the desired behavior of this Unigram wrapper.
        # SentencePieceProcessor's encode method can take `add_bos`, `add_eos` directly.
        # Let's simplify and assume our BaseTokenizer truncation/padding will handle max_length.
        
        _sp = self.special_tokens
        current_input_ids = list(ids_sequence_1)
        current_token_types = [0] * len(ids_sequence_1)
        current_tokens = list(tokens_sequence_1) if tokens_sequence_1 else None

        if ids_sequence_2 is not None: # Pair text
            if add_special_tokens and _sp.cls_token_id is not None and _sp.sep_token_id is not None: # BERT-like pair
                final_input_ids.append(_sp.cls_token_id)
                final_token_type_ids.append(0)
                final_special_tokens_mask.append(1)
                if return_tokens: final_output_tokens_list.append(_sp.cls_token)
                
                final_input_ids.extend(ids_sequence_1)
                final_token_type_ids.extend([0] * len(ids_sequence_1))
                final_special_tokens_mask.extend([0] * len(ids_sequence_1))
                if return_tokens and tokens_sequence_1: final_output_tokens_list.extend(tokens_sequence_1)

                final_input_ids.append(_sp.sep_token_id)
                final_token_type_ids.append(0)
                final_special_tokens_mask.append(1)
                if return_tokens: final_output_tokens_list.append(_sp.sep_token)

                final_input_ids.extend(ids_sequence_2)
                final_token_type_ids.extend([1] * len(ids_sequence_2))
                final_special_tokens_mask.extend([0] * len(ids_sequence_2))
                if return_tokens and tokens_sequence_2: final_output_tokens_list.extend(tokens_sequence_2)

                final_input_ids.append(_sp.sep_token_id)
                final_token_type_ids.append(1)
                final_special_tokens_mask.append(1)
                if return_tokens: final_output_tokens_list.append(_sp.sep_token)
            else: # Simple concatenation for pair if no CLS/SEP
                final_input_ids = ids_sequence_1 + ids_sequence_2
                final_token_type_ids = [0] * len(ids_sequence_1) + [1] * len(ids_sequence_2)
                final_special_tokens_mask = [0] * len(final_input_ids)
                if return_tokens and tokens_sequence_1 and tokens_sequence_2: final_output_tokens_list = tokens_sequence_1 + tokens_sequence_2
                elif return_tokens and tokens_sequence_1: final_output_tokens_list = tokens_sequence_1
                elif return_tokens and tokens_sequence_2: final_output_tokens_list = tokens_sequence_2
        else: # Single text
            if add_special_tokens and _sp.bos_token_id is not None:
                final_input_ids.append(_sp.bos_token_id)
                final_token_type_ids.append(0)
                final_special_tokens_mask.append(1)
                if return_tokens: final_output_tokens_list.append(_sp.bos_token)
            
            final_input_ids.extend(ids_sequence_1)
            final_token_type_ids.extend([0] * len(ids_sequence_1))
            final_special_tokens_mask.extend([0] * len(ids_sequence_1))
            if return_tokens and tokens_sequence_1: final_output_tokens_list.extend(tokens_sequence_1)

            if add_special_tokens and _sp.eos_token_id is not None:
                final_input_ids.append(_sp.eos_token_id)
                final_token_type_ids.append(0)
                final_special_tokens_mask.append(1)
                if return_tokens: final_output_tokens_list.append(_sp.eos_token)
        
        # Truncation and Padding using BaseTokenizer methods
        truncated_ids, truncated_tt_ids, overflow = self.apply_truncation(final_input_ids, final_token_type_ids)
        
        if return_tokens and final_output_tokens_list and len(final_output_tokens_list) > len(truncated_ids):
            final_output_tokens_list = final_output_tokens_list[:len(truncated_ids)]
        if len(final_special_tokens_mask) > len(truncated_ids):
            final_special_tokens_mask = final_special_tokens_mask[:len(truncated_ids)]

        padded_ids, attention_mask, padded_tt_ids = self.apply_padding(truncated_ids, token_type_ids=truncated_tt_ids)

        if return_tokens and final_output_tokens_list and len(final_output_tokens_list) < len(padded_ids):
            pad_token_str = _sp.pad_token or "<pad>"
            final_output_tokens_list.extend([pad_token_str] * (len(padded_ids) - len(final_output_tokens_list)))
        if len(final_special_tokens_mask) < len(padded_ids):
            final_special_tokens_mask.extend([0] * (len(padded_ids) - len(final_special_tokens_mask)))

        return TokenizedOutput(
            input_ids=padded_ids,
            attention_mask=attention_mask if self.options.return_attention_mask else None,
            token_type_ids=padded_tt_ids if self.options.return_token_type_ids else None,
            special_tokens_mask=final_special_tokens_mask if self.options.return_special_tokens_mask else None,
            overflowing_tokens=overflow if self.options.return_overflowing_tokens else None,
            length=len(padded_ids) if self.options.return_length else None,
            tokens=final_output_tokens_list
        )

    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        if not self.is_trained or not self.model:
            raise UntrainedTokenizerError("Unigram tokenizer is not trained or model not loaded.")

        # Filter special tokens before decoding if requested
        # SentencePiece decode handles its own special tokens like <s>, </s> well if they are part of `token_ids`.
        # However, if `skip_special_tokens` is True, we should respect it based on our `SpecialTokens` handler.
        ids_to_decode = token_ids
        if skip_special_tokens and self.special_tokens:
            all_sp_ids = self.special_tokens.all_special_ids
            ids_to_decode = [id_val for id_val in token_ids if id_val not in all_sp_ids]

        try:
            return self.model.decode_ids(ids_to_decode)
        except Exception as e:
            logger.error(f"Error decoding IDs with SentencePiece model: {e}")
            # Fallback or re-raise
            return "[decode_error]"

    def save(self, path: Union[str, Path], model_filename: str = "unigram_tokenizer.model", **kwargs) -> None:
        """Saves the Unigram tokenizer (model file and config)."""
        if not self.is_trained or not self.model:
            raise TokenizationError("Cannot save Unigram tokenizer: not trained or model not loaded.")
        
        save_path = Path(path)
        save_path.mkdir(parents=True, exist_ok=True)

        # SentencePiece model is typically saved as a single .model file.
        # If we trained it, it's already saved (e.g., {model_prefix}.model).
        # If loaded, we need a way to re-save it or copy the existing model file.
        # SentencePieceProcessor doesn't have a direct save/write method for an *already loaded* model.
        # So, we must ensure the model_file is copied to the save_path.
        # This assumes self.model.serialized_model_proto() or similar could be used if available,
        # or we copy the file from which it was loaded if `model_path` was stored.
        
        # For now, we assume that if trained, model is at model_prefix.model
        # If loaded, we'd need to know original path to copy it, or expect user to manage model file.
        # Let's require `model_path_to_copy_from` if trying to save a loaded model elsewhere.
        # Or, the `save` operation for Unigram simply means saving the *config* that points to an existing model file.
        
        # Simplified: config saves reference to model, model file should exist at that reference.
        # We are not copying the .model file here, just saving config.
        # The .model file should be alongside the config.json.
        final_model_path = save_path / model_filename
        logger.info(f"Unigram model file expected at: {final_model_path}. Ensure it's copied there if not generated by training.")

        config_to_save = {
            "tokenizer_type": self.tokenizer_type.value,
            "vocab_size": self.vocab_size,
            "model_file": model_filename, # Relative path to model file within the saved directory
            "special_tokens": self.special_tokens.get_special_tokens_dict() if self.special_tokens else {},
            "options": self.options.dict() if self.options else {},
            # Add normalizer/preprocessor configs if they are serializable
        }
        with open(save_path / "tokenizer_config.json", "w", encoding="utf-8") as f:
            json.dump(config_to_save, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Saved Unigram tokenizer configuration to {save_path}. Corresponding model file: {model_filename}")

    @classmethod
    def load(cls, path: Union[str, Path], **kwargs) -> "UnigramTokenizer":
        load_path = Path(path)
        config_file = load_path / "tokenizer_config.json"
        if not config_file.exists():
            raise FileNotFoundError(f"Tokenizer configuration (tokenizer_config.json) not found in {load_path}")
        
        with open(config_file, "r", encoding="utf-8") as f:
            config = json.load(f)
        
        model_filename = config.get("model_file", "unigram_tokenizer.model") # Default if not in config
        model_file_to_load = load_path / model_filename
        if not model_file_to_load.exists():
            raise FileNotFoundError(f"Unigram model file '{model_filename}' not found in {load_path}")

        st_config = config.get("special_tokens", {})
        opts_config = config.get("options", {})

        # Reconstruct SpecialTokens handler (basic example)
        special_tokens_handler = SpecialTokens()
        special_tokens_handler.register_special_tokens(st_config)
        
        options_obj = TokenizerOptions(**opts_config)

        # Normalizer and Preprocessor deserialization would go here if saved

        # Initialize the tokenizer, then explicitly load the model using its path
        tokenizer = cls(
            vocab_size=config.get("vocab_size"), # Will be updated by load_model
            special_tokens=special_tokens_handler,
            options=options_obj,
            # Do not pass model_path here to avoid double load, call load_model directly
        )
        tokenizer.load_model(model_file_to_load) # This sets up encoder, vocab_size, _is_trained
        
        logger.info(f"Loaded Unigram tokenizer from {load_path} with model {model_filename}")
        return tokenizer

__all__ = ["UnigramTokenizer"] 